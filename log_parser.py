import os
import sys
import re
import json
import hashlib
import time
from collections import defaultdict

# ==============================================================================
# [Dependency Check]
# ==============================================================================
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.cluster import DBSCAN
    import numpy as np
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False

# ==============================================================================
# 1. Template Manager
# ==============================================================================
class RuleTemplateManager:
    def __init__(self, template_file):
        self.template_dict = {} 
        self.var_pattern = re.compile(r"'(.*?)'")
        
        if template_file:
            print(f"ğŸ“‚ Loading Rule Templates from: {template_file}")
            self._load_templates(template_file)

    def _get_pure_template(self, text):
        # 1. ë³€ìˆ˜ ì˜ì—­ ë³´í˜¸
        temp = self.var_pattern.sub("'<VAR>'", text)
        # 2. ë…ë¦½ëœ ìˆ«ìë§Œ ë§ˆìŠ¤í‚¹
        temp = re.sub(r"\b\d+\b", "<NUM>", temp)
        return temp.strip()

    def _load_templates(self, file_path):
        if not os.path.exists(file_path):
            return
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith(('-', 'Rule', 'Severity')): continue
                parts = line.split(maxsplit=3)
                if len(parts) < 4: continue
                rule_id, message = parts[0], parts[3]
                pure_temp = self._get_pure_template(message)
                self.template_dict[pure_temp] = rule_id

    def get_rule_id(self, log_template):
        return self.template_dict.get(log_template, f"UNKNOWN_{hashlib.md5(log_template.encode()).hexdigest()[:6].upper()}")

# ==============================================================================
# 2. Parser
# ==============================================================================
class SubutaiParser:
    def __init__(self, template_manager):
        self.var_pattern = re.compile(r"'(.*?)'")
        self.tm = template_manager
        self.delimiters = [('/', 1), ('_', 2), ('-', 3)]  # (delimiter, priority)
    
    def extract_variable_stems(self, variable):
        """
        Extract semantic stems from variable respecting delimiter priority.
        Priority: '/' (highest) > '_' > '-' (lowest)
        
        Strategy: Split by priority delimiters but keep meaningful components.
        - '/' is hierarchy separator: splits into distinct components
        - '_' is compound separator within a component: may keep together or split
        - '-' is sub-component separator: splits into atoms
        
        Example: 'BLK_CPU/A/B/C/mem_top_ABC' -> ['BLK_CPU', 'A', 'B', 'C', 'mem_top', 'ABC']
        Example: 'mem_top_ABC' -> ['mem_top', 'ABC']
        
        Returns list of stem components in hierarchical order.
        """
        if not variable:
            return []
        
        # Step 1: Split by highest priority delimiter ('/')
        parts = variable.split('/')
        stems = []
        
        for part in parts:
            if not part:
                continue
            
            # Step 2: For each part, decide whether to split by '_' or '-'
            # Strategy: If the part is a known hierarchy marker (A, B, C, X, Y, etc.) or very short, keep it
            # Otherwise split by '_' (compound names like mem_top), then by '-'
            
            if len(part) <= 3 or part.isupper():
                # Single letters or uppercase markers like BLK, CPU, SENSOR - keep as one stem
                stems.append(part)
            else:
                # Compound names: split by '_' first, then '-'
                sub_parts = part.split('_')
                for sub_part in sub_parts:
                    if sub_part:
                        # Final split by '-' for components like 'ABC', '123-456'
                        final_parts = sub_part.split('-')
                        stems.extend([p for p in final_parts if p])
        
        return stems

    def parse_line(self, line):
        line = line.strip()
        if not line: return None
        
        if re.search(r'\b\d+\s+of\s+\d+\b', line):
            pass 
        else:
            return None

        line = " ".join(line.split()[4:])

        variables = self.var_pattern.findall(line)
        var_tuple = tuple(variables) if variables else ("NO_VAR",)
        
        # Extract variable stems for hierarchical grouping
        variable_stems = []
        if var_tuple and var_tuple != ("NO_VAR",):
            for var in var_tuple:
                stems = self.extract_variable_stems(var)
                variable_stems.extend(stems)
        
        stems_tuple = tuple(variable_stems) if variable_stems else ("NO_STEM",)
        
        template = self.tm._get_pure_template(line)
        rule_id = self.tm.get_rule_id(template)
        
        return {
            "rule_id": rule_id,
            "variables": var_tuple,
            "variable_stems": stems_tuple,
            "template": template,
            "raw_log": line  # <--- ì›ë³¸ ë¡œê·¸ ì €ì¥ë¨
        }

# ==============================================================================
# 3. Logic Clusterer
# ==============================================================================
class LogicClusterer:
    def get_logic_signature(self, var_tuple):
        if not var_tuple or var_tuple == ("NO_VAR",): return "NO_VAR"
        sigs = [re.sub(r"\d+", "*", str(v)) for v in var_tuple]
        return " / ".join(sigs)
    
    def get_stem_signature(self, stem_tuple):
        """
        Create signature from variable stems, replacing numbers with wildcards.
        Stems are already decomposed, so this focuses on semantic components.
        
        Example: ('mem_top', 'ABC', 'value', '123') -> 'mem_top ABC value *'
        """
        if not stem_tuple or stem_tuple == ("NO_STEM",): 
            return "NO_STEM"
        
        # Replace numeric stems with wildcard, keep semantic stems
        sigs = []
        for stem in stem_tuple:
            if stem.isdigit():
                sigs.append("*")
            else:
                # Keep non-numeric stems as-is (they're already atomic)
                sigs.append(stem)
        
        return " ".join(sigs)

    def run(self, parsed_logs):
        groups = defaultdict(list)
        for p in parsed_logs:
            # [1ì°¨ ê·¸ë£¹í•‘] ì›ë˜ ë°©ì‹: variablesë§Œ ì‚¬ìš© (stem ë¬´ì‹œ)
            full_sig = self.get_logic_signature(p['variables'])
            key = (p['rule_id'], full_sig, p['template'])
            groups[key].append(p)

        results = []
        for (rule_id, full_sig, temp), members in groups.items():
            results.append({
                "type": "LogicGroup",
                "rule_id": rule_id,
                "pattern": full_sig,  # ì›ë³¸ ë°©ì‹: ë³€ìˆ˜ ê¸°ë°˜ íŒ¨í„´
                "template": temp,
                "count": len(members),
                "members": members  # <--- ì—¬ê¸°ì— raw_logê°€ í¬í•¨ëœ íŒŒì‹± ê°ì²´ë“¤ì´ ìˆìŒ
            })
        results.sort(key=lambda x: x['count'], reverse=True)
        return results

# ==============================================================================
# 4. AI Clusterer
# ==============================================================================
class AIClusterer:
    def __init__(self, model_path='all-MiniLM-L6-v2', config_file='rule_clustering_config.json'):
        global AI_AVAILABLE
        if AI_AVAILABLE:
            try:
                self.model = SentenceTransformer(model_path)
            except:
                AI_AVAILABLE = False
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ruleë³„ epsì™€ tail_weight ë¡œë“œ
        self.rule_config = self._load_config(config_file)
        
        # ê¸°ë³¸ ì„¤ì • (ëª¨ë“  rule ì ìš©)
        self.default_eps = 0.2
        self.default_tail_weight = 2

    def _load_config(self, config_file):
        """ì„¤ì • íŒŒì¼ì—ì„œ ruleë³„ íŒŒë¼ë¯¸í„° ë¡œë“œ"""
        if not os.path.exists(config_file):
            print(f"   âš ï¸  Config file '{config_file}' not found. Using default settings.")
            return {}
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"   âœ… Loaded rule config from '{config_file}'")
            return config.get('rules', {})
        except Exception as e:
            print(f"   âš ï¸  Error loading config: {e}. Using default settings.")
            return {}

    def get_rule_config(self, rule_id):
        """Ruleë³„ ì„¤ì • ì¡°íšŒ, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜"""
        if rule_id in self.rule_config:
            config = self.rule_config[rule_id].copy()
            if 'eps' not in config:
                config['eps'] = self.default_eps
            if 'tail_weight' not in config:
                config['tail_weight'] = self.default_tail_weight
            if 'tail_levels' not in config:
                config['tail_levels'] = 1  # ê¸°ë³¸: ë’·ë¶€ë¶„ 1ê°œ ë ˆë²¨
            if 'tail_weights' not in config:
                config['tail_weights'] = None  # ê¸°ë³¸: ê· ë“± ê°€ì¤‘ì¹˜
            if 'variable_position_weights' not in config:
                config['variable_position_weights'] = None  # ê¸°ë³¸: ë³€ìˆ˜ ìœ„ì¹˜ ê°€ì¤‘ì¹˜ ì—†ìŒ
            if 'variable_value_weights' not in config:
                config['variable_value_weights'] = None  # ê¸°ë³¸: ë³€ìˆ˜ ê°’ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—†ìŒ
            return config
        return {
            'eps': self.default_eps,
            'tail_weight': self.default_tail_weight,
            'tail_levels': 1,
            'tail_weights': None,
            'variable_position_weights': None,
            'variable_value_weights': None
        }

    def extract_variable_tail(self, full_pattern, tail_levels=1, tail_weights=None, variable_position_weights=None):
        """
        VLSI ë³€ìˆ˜ì˜ ë’·ë¶€ë¶„ ì¶”ì¶œ (ë’·ë¶€ë¶„ì´ ë” ì¤‘ìš”í•¨)
        ë³€ìˆ˜ ìœ„ì¹˜ë³„ ê°€ì¤‘ì¹˜ë„ ì§€ì›
        
        Args:
            full_pattern: 'BLK_CPU/A/B/C/mem_top_ABC' í˜•íƒœ
            tail_levels: ë’·ë¶€ë¶„ ëª‡ ê°œ ë ˆë²¨ì„ ì¶”ì¶œí• ì§€ (ê¸°ë³¸ 1)
            tail_weights: ê° ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸
                          ì˜ˆ: [2, 3] â†’ ë§ˆì§€ë§‰ì€ 2ë°°, ê·¸ ì•ì€ 3ë°°
            variable_position_weights: ë³€ìˆ˜ ìœ„ì¹˜ë³„ ê°€ì¤‘ì¹˜
                          ì˜ˆ: [3, 2, 1] â†’ ì²«ë²ˆì§¸ ë³€ìˆ˜ëŠ” 3ë°°, ë‘˜ì§¸ëŠ” 2ë°°, ì…‹ì§¸ëŠ” 1ë°°
        
        Returns:
            ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ë’·ë¶€ë¶„ ë¬¸ìì—´
        
        Example:
            full_pattern = 'BLK_CPU/A/B/C/mem_top_ABC'
            
            tail_levels=1, tail_weights=[2]
            â†’ 'ABC ABC'
            
            tail_levels=2, tail_weights=[3, 2]
            â†’ 'mem_top mem_top mem_top ABC ABC'
            
            ë³€ìˆ˜ íŠœí”Œì´ ('var1', 'var2', 'var3')ì´ê³ 
            variable_position_weights=[3, 2, 1]ì´ë©´
            â†’ 'var1 var1 var1 var2 var2 var3'
        """
        if ' / ' not in full_pattern:
            return full_pattern
        
        parts = full_pattern.split(' / ')
        
        # ë’·ë¶€ë¶„ ë ˆë²¨ ì¶”ì¶œ
        tail_parts = parts[-tail_levels:] if tail_levels <= len(parts) else parts
        
        # ê°€ì¤‘ì¹˜ ì„¤ì • (ê¸°ë³¸ê°’: ëª¨ë‘ 1)
        if tail_weights is None:
            tail_weights = [1] * len(tail_parts)
        else:
            # tail_weightsê°€ ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            while len(tail_weights) < len(tail_parts):
                tail_weights.append(tail_weights[-1] if tail_weights else 1)
        
        # ê° ë¶€ë¶„ì„ ê°€ì¤‘ì¹˜ë§Œí¼ ë°˜ë³µ
        result = []
        for part, weight in zip(tail_parts, tail_weights):
            result.extend([part] * weight)
        
        # ë³€ìˆ˜ ìœ„ì¹˜ë³„ ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì ìš©
        if variable_position_weights:
            result = self._apply_variable_position_weights(result, variable_position_weights)
        
        return ' '.join(result)
    
    def _apply_variable_position_weights(self, parts, variable_position_weights):
        """
        ë³€ìˆ˜ ìœ„ì¹˜ë³„ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ë¶„ ë¬¸ìì—´ë“¤ì— ì ìš©
        ì˜ˆ: parts=['mem_top', 'ABC'], variable_position_weights=[3, 2]
        â†’ ['mem_top', 'mem_top', 'mem_top', 'ABC', 'ABC']
        """
        if not parts or not variable_position_weights:
            return parts
        
        result = []
        for i, part in enumerate(parts):
            # ìœ„ì¹˜ë³„ ê°€ì¤‘ì¹˜ ì¡°íšŒ (ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ ê°’ ì‚¬ìš©)
            weight_idx = min(i, len(variable_position_weights) - 1)
            weight = variable_position_weights[weight_idx]
            result.extend([part] * weight)
        
        return result
    
    def _apply_variable_value_weights(self, variables, variable_value_weights):
        """
        ë³€ìˆ˜ ê°’ ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¥¼ ì ìš© (ë³€ìˆ˜ ê°’ ìì²´ë³„ë¡œ ê°€ì¤‘ì¹˜ ì„¤ì •)
        ì˜ˆ: variables=['pipe_4', 'pipe_5', 'pipe_5']
             variable_value_weights={'pipe_4': 3, 'pipe_5': 1}
        â†’ ['pipe_4', 'pipe_4', 'pipe_4', 'pipe_5', 'pipe_5']
        """
        if not variables or not variable_value_weights:
            return variables
        
        result = []
        for var in variables:
            # ë³€ìˆ˜ ê°’ì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ ì¡°íšŒ (ì—†ìœ¼ë©´ 1)
            weight = variable_value_weights.get(var, 1)
            result.extend([var] * weight)
        
        return result

    def run(self, logic_groups):
        if not AI_AVAILABLE or not logic_groups: return []

        print(f"ğŸ¤– Stage 2 - AI Clustering: analyzing {len(logic_groups)} logic groups...")
        
        # rule_idë³„ë¡œ ê·¸ë£¹ ë¶„ë¥˜
        groups_by_rule = defaultdict(list)
        for g in logic_groups:
            groups_by_rule[g['rule_id']].append(g)
        
        print(f"   Grouping by rule_id: {len(groups_by_rule)} different rules")
        
        final_output = []
        ai_group_counter = 0
        
        # Ruleë³„ë¡œ ë”°ë¡œ AI Clustering ìˆ˜í–‰
        for rule_id, rule_groups in groups_by_rule.items():
            config = self.get_rule_config(rule_id)
            eps = config['eps']
            tail_weight = config['tail_weight']
            tail_levels = config.get('tail_levels', 1)
            tail_weights = config.get('tail_weights', None)
            variable_position_weights = config.get('variable_position_weights', None)
            variable_value_weights = config.get('variable_value_weights', None)
            
            if len(rule_groups) < 2:
                # ê·¸ë£¹ì´ 1ê°œë©´ ë³‘í•©í•  ê²ƒì´ ì—†ìŒ
                for g in rule_groups:
                    ai_group_counter += 1
                    all_raw_logs = [m['raw_log'] for m in g['members']]
                    final_output.append({
                        "type": "AISuperGroup",
                        "super_group_id": f"{rule_id}_SG_{ai_group_counter}",
                        "rule_id": rule_id,
                        "representative_template": g['template'],
                        "representative_pattern": g['pattern'],
                        "total_count": g['count'],
                        "merged_variants_count": 1,
                        "original_logs": all_raw_logs
                    })
                continue
            
            # ë™ì¼ rule_id ë‚´ì—ì„œë§Œ embedding ë° clustering
            # VLSI ë³€ìˆ˜ì˜ ë’·ë¶€ë¶„(ì‹¤ì œ ì¤‘ìš” ì •ë³´)ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ê¸° ìœ„í•´
            embedding_inputs = []
            for g in rule_groups:
                # ë’·ë¶€ë¶„ ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜ + ë³€ìˆ˜ ìœ„ì¹˜ë³„ ê°€ì¤‘ì¹˜ + ë³€ìˆ˜ ê°’ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
                if tail_weights:
                    # ì„¤ì • íŒŒì¼ì—ì„œ ì§€ì •í•œ ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜ ì‚¬ìš©
                    tail_text = self.extract_variable_tail(g['pattern'], tail_levels, tail_weights, variable_position_weights)
                else:
                    # ë‹¨ìˆœ ë°˜ë³µ (tail_weight ì‚¬ìš©)
                    tail = self.extract_variable_tail(g['pattern'], tail_levels, None, variable_position_weights)
                    tail_text = ' '.join([tail] * tail_weight)
                
                # ë³€ìˆ˜ ê°’ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš© (tail_textë¥¼ ë‹¤ì‹œ ê°€ì¤‘ì¹˜ ì²˜ë¦¬)
                if variable_value_weights:
                    # tail_textëŠ” ì´ë¯¸ ì²˜ë¦¬ëœ ë³€ìˆ˜ë“¤ì´ë¯€ë¡œ, patternì—ì„œ ì¶”ì¶œí•œ ë³€ìˆ˜ë¡œ ì²˜ë¦¬
                    # patternì—ì„œ ë³€ìˆ˜ ì¶”ì¶œ
                    var_pattern = re.compile(r"'(.*?)'")
                    # g['pattern']ì—ëŠ” " / " í˜•íƒœë¡œ ê²½ë¡œê°€ ìˆìŒ
                    pattern_text = g['pattern'].replace(' / ', ' ')
                    variables = var_pattern.findall(pattern_text)
                    
                    # ë³€ìˆ˜ ê°’ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
                    weighted_vars = self._apply_variable_value_weights(variables, variable_value_weights)
                    var_text = ' '.join(weighted_vars)
                    embedding_input = f"{g['template']} {var_text}"
                else:
                    embedding_input = f"{g['template']} {tail_text}"
                
                embedding_inputs.append(embedding_input)
            
            embeddings = self.model.encode(embedding_inputs, batch_size=128, show_progress_bar=False)
            
            # Ruleë³„ ì„¤ì •ëœ epsë¡œ clustering
            clustering = DBSCAN(eps=eps, min_samples=1, metric='cosine').fit(embeddings)
            
            ai_grouped = defaultdict(lambda: {"total_count": 0, "logic_subgroups": []})
            for label, logic_group in zip(clustering.labels_, rule_groups):
                cluster_key = f"{rule_id}_SG_{label}"
                ai_grouped[cluster_key]["total_count"] += logic_group['count']
                ai_grouped[cluster_key]["logic_subgroups"].append(logic_group)

            # ê²°ê³¼ ìƒì„±
            for key, data in ai_grouped.items():
                ai_group_counter += 1
                main = max(data["logic_subgroups"], key=lambda x: x['count'])
                
                all_raw_logs = []
                for sub in data["logic_subgroups"]:
                    for member in sub["members"]:
                        all_raw_logs.append(member["raw_log"])

                final_output.append({
                    "type": "AISuperGroup",
                    "super_group_id": key,
                    "rule_id": rule_id,
                    "representative_template": main['template'],
                    "representative_pattern": main['pattern'],
                    "total_count": data["total_count"],
                    "merged_variants_count": len(data["logic_subgroups"]),
                    "original_logs": all_raw_logs
                })
        
        final_output.sort(key=lambda x: x['total_count'], reverse=True)
        return final_output

# ==============================================================================
# 5. Main Execution
# ==============================================================================
if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python subutai_reviewer.py <LOG_FILE> <TEMPLATE_FILE>")
        sys.exit(1)

    log_file = sys.argv[1]
    rule_file = sys.argv[2]

    # 1. Parsing
    tm = RuleTemplateManager(rule_file)
    parser = SubutaiParser(tm)
    parsed_logs = []
    
    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            stripped = line.strip()
            if not stripped or stripped.startswith(('-', '=', 'Rule', 'Severity')): continue
            res = parser.parse_line(stripped)
            if res: parsed_logs.append(res)

    # 2. Logic Clustering [1ì°¨ ê·¸ë£¹í•‘: ì›ë˜ ë°©ì‹]
    logic_results = LogicClusterer().run(parsed_logs)
    print(f"\nğŸ“Š Stage 1 - Logic Clustering (Original Method - Variables Only):")
    print(f"   Input logs: {len(parsed_logs):,}")
    print(f"   Output groups: {len(logic_results):,}")
    print(f"   Compression ratio: {len(parsed_logs) / len(logic_results):.2f}x")

    # 3. AI Clustering [2ì°¨ ê·¸ë£¹í•‘: ì˜ë¯¸ì  ë³‘í•©]
    results = [] # <--- ì—¬ê¸°ì— ëª¨ë“  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

    if AI_AVAILABLE:
        # 2ì°¨ ê·¸ë£¹í•‘: 1ì°¨ ë¡œì§ ê·¸ë£¹ë“¤ì„ AIë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ì¬ë³‘í•©
        results = AIClusterer().run(logic_results)
        print(f"\nğŸ¤– Stage 2 - AI Clustering (Semantic Merging of 1st-Groups):")
        print(f"   Input 1st-groups: {len(logic_results):,}")
        print(f"   Output 2nd-groups: {len(results):,}")
        print(f"   Final compression ratio: {len(parsed_logs) / len(results):.2f}x")
    else:
        # AIê°€ ì—†ìœ¼ë©´ Logic ê²°ê³¼ë§Œ ë°˜í™˜
        for g in logic_results:
            # Logic ê·¸ë£¹ì˜ ì›ë³¸ ë¡œê·¸ ë³µêµ¬
            raw_logs = [m['raw_log'] for m in g['members']]
            results.append({
                "type": "LogicGroup",
                "rule_id": g['rule_id'],
                "representative_pattern": g['pattern'],
                "total_count": g['count'],
                "original_logs": raw_logs
            })

    # 4. ê²°ê³¼ ì¶œë ¥ ë° íŒŒì¼ ì €ì¥
    print("\n" + "="*80)
    print(f"âœ… Final Results: {len(results)} Groups Created.")
    print("="*80)
    
    # í™”ë©´ ì¶œë ¥ (ìƒ˜í”Œ)
    for i, res in enumerate(results[:5]):
        pattern_display = res.get('representative_pattern', 'N/A')
        merged_info = f" (merged {res.get('merged_variants_count', 1)} groups)" if res.get('merged_variants_count', 1) > 1 else ""
        print(f"{i+1:02d}. [{res['rule_id']}] {pattern_display}{merged_info}")
        print(f"    Count: {res['total_count']:,}")
        print(f"    Original Logs Sample (Top 2):")
        for log in res['original_logs'][:2]:
            print(f"      - {log}")
        print("-" * 60)

    # ê²°ê³¼ íŒŒì¼ ì €ì¥ (JSON)
    output_filename = "subutai_results.json"
    with open(output_filename, "w", encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ëª¨ë“  ê²°ê³¼(ì›ë³¸ ë¡œê·¸ í¬í•¨)ê°€ '{output_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")