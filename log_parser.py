import os
import sys
import re
import json
import time
from collections import defaultdict

# ==============================================================================
# [Dependency Check]
# ==============================================================================
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.cluster import DBSCAN
    import numpy as np
    AI_AVAILABLE = True
except ImportError:
    print("âš ï¸  [Warning] AI ë¼ì´ë¸ŒëŸ¬ë¦¬(sentence-transformers)ê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("   AI Clustering ë‹¨ê³„ëŠ” ìŠ¤í‚µë˜ê³  Logic ë‹¨ê³„ ê²°ê³¼ë§Œ ì¶œë ¥ë©ë‹ˆë‹¤.")
    AI_AVAILABLE = False

# ==============================================================================
# 1. Log Reader & Parser
# ==============================================================================
class SubutaiLogReader:
    def __init__(self, file_path):
        self.file_path = file_path

    def _is_ignorable(self, line):
        line = line.strip()
        if not line or line.startswith(("---", "===", "Info:", "Page")): return True
        return False

    def stream_valid_lines(self):
        if not os.path.exists(self.file_path):
            print(f"âŒ Error: File not found ({self.file_path})")
            sys.exit(1)
            
        with open(self.file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if not self._is_ignorable(line): yield line.strip()

class SubutaiParser:
    def __init__(self):
        self.rule_pattern = re.compile(r"^([A-Z]+-\d+)")
        self.var_pattern = re.compile(r"['\"](.*?)['\"]")

    def parse_line(self, line):
        match = self.rule_pattern.search(line)
        rule_id = match.group(1) if match else "UNKNOWN"
        variables = self.var_pattern.findall(line)
        # í…œí”Œë¦¿: ë³€ìˆ˜ ë‚´ìš© ì œê±°
        template = self.var_pattern.sub("'<VAR>'", line)
        template = re.sub(r"\d+", "<NUM>", template)
        return {
            "rule_id": rule_id,
            "template": template,
            "variables": variables,
            "raw_log": line
        }

# ==============================================================================
# 2. Logic Layer: Full Path Preservation (ìˆ«ìë§Œ ë§ˆìŠ¤í‚¹)
# ==============================================================================
class LogicClusterer:
    def __init__(self):
        pass

    def get_logic_signature(self, var_str):
        # ì „ì²´ ê²½ë¡œë¥¼ ìœ ì§€í•˜ë˜, ìˆ«ìë§Œ *ë¡œ ì¹˜í™˜
        # top/u_cpu_0/wire -> top/u_cpu_*/wire
        return re.sub(r"\d+", "*", var_str)

    def run(self, parsed_logs):
        groups = defaultdict(list)
        
        for p in parsed_logs:
            if not p['variables']:
                sig = "NO_VAR"
            else:
                sig = self.get_logic_signature(p['variables'][0])
            
            key = (p['rule_id'], sig)
            groups[key].append(p)

        logic_results = []
        for (rule_id, sig), members in groups.items():
            logic_results.append({
                "rule_id": rule_id,
                "pattern": sig,
                "count": len(members),
                "template": members[0]['template'],
                "sample_log": members[0]['raw_log']
            })
            
        return logic_results

# ==============================================================================
# 3. AI Layer: Semantic Clusterer
# ==============================================================================
class AIClusterer:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        if AI_AVAILABLE:
            print(f"â³  [System] Loading AI Model ({model_name})...")
            self.model = SentenceTransformer(model_name)
            print("âœ…  [System] AI Model Loaded.")

    def run(self, logic_groups):
        if not AI_AVAILABLE or not logic_groups: return logic_groups

        print(f"ğŸ¤–  [System] AI analyzing {len(logic_groups)} patterns...")
        t0 = time.time()

        # Input ìƒì„±
        embedding_inputs = [f"{g['rule_id']} {g['pattern']}" for g in logic_groups]

        # ë²¡í„°í™” & í´ëŸ¬ìŠ¤í„°ë§
        embeddings = self.model.encode(embedding_inputs, batch_size=128, show_progress_bar=False)
        clustering = DBSCAN(eps=0.25, min_samples=2, metric='cosine').fit(embeddings)
        labels = clustering.labels_

        # ê²°ê³¼ ë³‘í•©
        ai_grouped_result = defaultdict(lambda: {
            "super_group_id": None, "total_count": 0, 
            "representative_pattern": "", "sub_patterns": []
        })

        for label, logic_group in zip(labels, logic_groups):
            # Noise(-1)ëŠ” ê°œë³„ ì²˜ë¦¬
            cluster_key = f"SG_{label}" if label != -1 else f"NOISE_{logic_group['pattern']}"
            
            group_data = ai_grouped_result[cluster_key]
            group_data["super_group_id"] = cluster_key
            group_data["total_count"] += logic_group['count']
            group_data["sub_patterns"].append(logic_group)

        # ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        final_output = []
        for key, data in ai_grouped_result.items():
            main_sub = max(data["sub_patterns"], key=lambda x: x['count'])
            data["representative_pattern"] = main_sub["pattern"]
            data["rule_id"] = main_sub["rule_id"]
            final_output.append(data)

        final_output.sort(key=lambda x: x['total_count'], reverse=True)
        print(f"âš¡  [System] AI Analysis done in {time.time()-t0:.2f}s")
        return final_output

# ==============================================================================
# 4. Main Execution
# ==============================================================================
if __name__ == "__main__":
    # 1. ì…ë ¥ í™•ì¸
    if len(sys.argv) < 2:
        print("Usage: python subutai_final.py <log_file_path>")
        sys.exit(1)
    
    log_file = sys.argv[1]
    print(f"\nğŸš€  Subutai AI Reviewer Started. Target: {log_file}")
    print("=" * 60)

    # --- Stage 0: Parse ---
    reader = SubutaiLogReader(log_file)
    parser = SubutaiParser()
    
    raw_lines = list(reader.stream_valid_lines())
    parsed_logs = [parser.parse_line(line) for line in raw_lines]
    
    print(f"\n[Stage 0] Parsing Completed")
    print(f"   - Input Lines (Valid): {len(raw_lines):,}")
    print(f"   - Parsed Elements    : {len(parsed_logs):,}")

    # --- Stage 1: Logic Clustering ---
    logic_engine = LogicClusterer()
    logic_results = logic_engine.run(parsed_logs)
    
    # í†µê³„ ê³„ì‚°
    logic_groups_cnt = len(logic_results)
    logic_total_elements = sum(g['count'] for g in logic_results)
    
    print(f"\n[Stage 1] Logic Clustering (Full Path Masking)")
    print(f"   - Groups Created     : {logic_groups_cnt:,}")
    print(f"   - Total Elements     : {logic_total_elements:,}")
    if logic_total_elements != len(parsed_logs):
        print("   âš ï¸  [Warning] Count Mismatch in Logic Stage!")

    # --- Stage 2: AI Clustering ---
    if AI_AVAILABLE:
        ai_engine = AIClusterer()
        final_results = ai_engine.run(logic_results)
        
        # í†µê³„ ê³„ì‚°
        ai_groups_cnt = len(final_results)
        ai_total_elements = sum(g['total_count'] for g in final_results)
        
        print(f"\n[Stage 2] AI Semantic Clustering (DBSCAN)")
        print(f"   - Super Groups       : {ai_groups_cnt:,}")
        print(f"   - Total Elements     : {ai_total_elements:,}")
        
        # ì••ì¶•ë¥  ê³„ì‚°
        compression_ratio = (1 - (ai_groups_cnt / len(parsed_logs))) * 100
        print(f"   - Compression Ratio  : {compression_ratio:.2f}%")
        
        if ai_total_elements != len(parsed_logs):
            print("   âš ï¸  [Warning] Count Mismatch in AI Stage!")
            
    else:
        final_results = logic_results
        print("\n[Stage 2] Skipped (AI Library Not Found)")

    # --- Final Report ---
    print("\n" + "=" * 60)
    print(f"ğŸ“Š  TOP 10 ISSUE GROUPS")
    print("=" * 60)
    
    for i, group in enumerate(final_results[:10]):
        # ëŒ€í‘œ íŒ¨í„´
        pat = group.get('representative_pattern', group.get('pattern'))
        # ê°¯ìˆ˜
        cnt = group.get('total_count', group.get('count'))
        # ì„œë¸Œ ê·¸ë£¹ ê°œìˆ˜ (AI ì¼ì„ ë•Œë§Œ ì¡´ì¬)
        merged_info = ""
        if 'sub_patterns' in group and len(group['sub_patterns']) > 1:
            merged_info = f"(Merged {len(group['sub_patterns'])} variants)"
            
        print(f"{i+1:02d}. [{pat}]")
        print(f"    Count: {cnt:,} {merged_info}")
        
        # ë³‘í•©ëœ í•˜ìœ„ íŒ¨í„´ ì˜ˆì‹œ ì¶œë ¥
        if 'sub_patterns' in group and len(group['sub_patterns']) > 1:
            # ìƒìœ„ 3ê°œë§Œ ë³´ì—¬ì¤Œ
            sorted_subs = sorted(group['sub_patterns'], key=lambda x: x['count'], reverse=True)
            for sub in sorted_subs[:3]:
                print(f"      â”” {sub['pattern']} (cnt: {sub['count']})")
            if len(sorted_subs) > 3:
                print(f"      â”” ... and {len(sorted_subs)-3} more")
        
        print("-" * 40)