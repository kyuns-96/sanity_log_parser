import os
import re
import json
import time
from collections import defaultdict

# ==============================================================================
# [Dependency Check]
# ==============================================================================
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.cluster import DBSCAN
    import numpy as np
    AI_AVAILABLE = True
except ImportError:
    print("âš ï¸ ê²½ê³ : AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜. (pip install sentence-transformers scikit-learn)")
    AI_AVAILABLE = False

# ==============================================================================
# 1. Log Reader & Parser
# ==============================================================================
class SubutaiLogReader:
    def __init__(self, file_path):
        self.file_path = file_path

    def _is_ignorable(self, line):
        line = line.strip()
        if not line or line.startswith(("---", "===", "Info:", "Page")): return True
        return False

    def stream_valid_lines(self):
        if not os.path.exists(self.file_path): return []
        with open(self.file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if not self._is_ignorable(line): yield line.strip()

class SubutaiParser:
    def __init__(self):
        self.rule_pattern = re.compile(r"^([A-Z]+-\d+)")
        self.var_pattern = re.compile(r"['\"](.*?)['\"]")

    def parse_line(self, line):
        match = self.rule_pattern.search(line)
        rule_id = match.group(1) if match else "UNKNOWN"
        variables = self.var_pattern.findall(line)
        # í…œí”Œë¦¿: ë³€ìˆ˜ ë‚´ìš© ì œê±°
        template = self.var_pattern.sub("'<VAR>'", line)
        template = re.sub(r"\d+", "<NUM>", template)
        return {
            "rule_id": rule_id,
            "template": template,
            "variables": variables,
            "raw_log": line
        }

# ==============================================================================
# 2. Logic Layer: Full Path Logic (ì ˆì‚­ ì—†ìŒ!)
# ==============================================================================
class LogicClusterer:
    def __init__(self):
        pass

    def get_logic_signature(self, var_str):
        """
        [ìˆ˜ì •ë¨] ê³„ì¸µ êµ¬ì¡°ë¥¼ ìë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ëŒ€ì‹  ìˆ«ì(Index)ë§Œ ë§ˆìŠ¤í‚¹í•˜ì—¬ ì „ì²´ ê²½ë¡œë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
        
        Input:  top/u_cpu_0/core/reg_128
        Output: top/u_cpu_*/core/reg_*
        """
        # ìˆ«ìë¥¼ ëª¨ë‘ *ë¡œ ì¹˜í™˜ (ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€)
        masked_path = re.sub(r"\d+", "*", var_str)
        return masked_path

    def run(self, parsed_logs):
        groups = defaultdict(list)
        
        for p in parsed_logs:
            if not p['variables']:
                sig = "NO_VAR"
            else:
                # ì²« ë²ˆì§¸ ë³€ìˆ˜ ê¸°ì¤€ (í•„ìš”ì‹œ Source/Dest ëª¨ë‘ ê³ ë ¤ ê°€ëŠ¥)
                sig = self.get_logic_signature(p['variables'][0])
            
            # Rule ID + Full Path Patternìœ¼ë¡œ 1ì°¨ ê·¸ë£¹í•‘
            key = (p['rule_id'], sig)
            groups[key].append(p)

        # AI ì—”ì§„ ì—°ë™ìš© ë°ì´í„° í¬ë§·
        logic_results = []
        for (rule_id, sig), members in groups.items():
            logic_results.append({
                "rule_id": rule_id,
                "pattern": sig,  # ì „ì²´ ê²½ë¡œê°€ ì‚´ì•„ìˆëŠ” íŒ¨í„´
                "count": len(members),
                "template": members[0]['template'],
                "sample_log": members[0]['raw_log']
            })
            
        return logic_results

# ==============================================================================
# 3. AI Layer: Semantic Clusterer
# ==============================================================================
class AIClusterer:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        if AI_AVAILABLE:
            print(f"â³ AI ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
            self.model = SentenceTransformer(model_name)
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    def run(self, logic_groups):
        if not AI_AVAILABLE or not logic_groups: return logic_groups

        print(f"ğŸ¤– AI ë¶„ì„ ì‹œì‘: {len(logic_groups)}ê°œì˜ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.")
        t0 = time.time()

        # Input: Rule ID + Full Path Pattern
        # ì˜ˆ: "LINT-01 top/u_cpu_*/core/reg_*"
        embedding_inputs = [f"{g['rule_id']} {g['pattern']}" for g in logic_groups]

        # ë²¡í„°í™”
        embeddings = self.model.encode(embedding_inputs, batch_size=128, show_progress_bar=True)

        # í´ëŸ¬ìŠ¤í„°ë§ (DBSCAN)
        # eps=0.25: ìœ ì‚¬ë„ ì•½ 75% ì´ìƒì´ë©´ ê°™ì€ ê·¸ë£¹
        clustering = DBSCAN(eps=0.25, min_samples=2, metric='cosine').fit(embeddings)
        labels = clustering.labels_

        # ê²°ê³¼ ë³‘í•©
        ai_grouped_result = defaultdict(lambda: {
            "super_group_id": None, "total_count": 0, 
            "representative_pattern": "", "sub_patterns": []
        })

        for label, logic_group in zip(labels, logic_groups):
            # Noise(-1)ëŠ” ê°œë³„ ê·¸ë£¹ìœ¼ë¡œ ì²˜ë¦¬
            cluster_key = f"SG_{label}" if label != -1 else f"NOISE_{logic_group['pattern']}"
            
            group_data = ai_grouped_result[cluster_key]
            group_data["super_group_id"] = cluster_key
            group_data["total_count"] += logic_group['count']
            group_data["sub_patterns"].append(logic_group)

        # ìµœì¢… ì •ë¦¬
        final_output = []
        for key, data in ai_grouped_result.items():
            # ê°€ì¥ ë¹ˆë„ ë†’ì€ íŒ¨í„´ì„ ëŒ€í‘œ ì´ë¦„ìœ¼ë¡œ
            main_sub = max(data["sub_patterns"], key=lambda x: x['count'])
            data["representative_pattern"] = main_sub["pattern"]
            data["rule_id"] = main_sub["rule_id"]
            final_output.append(data)

        final_output.sort(key=lambda x: x['total_count'], reverse=True)
        print(f"âš¡ AI ë¶„ì„ ì™„ë£Œ ({time.time()-t0:.2f}ì´ˆ)")
        return final_output

# ==============================================================================
# 4. Main Execution
# ==============================================================================
if __name__ == "__main__":
    # --- í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„± ---
    log_filename = "test_run.log"
    with open(log_filename, "w") as f:
        f.write("Info: Start\n")
        # [Case 1] ê²½ë¡œê°€ ê¹Šì§€ë§Œ ë‚´ìš©ì€ ìœ ì‚¬í•œ ê²½ìš° -> Logicì€ ë¶„ë¦¬í•˜ì§€ë§Œ AIê°€ ë¬¶ì–´ì•¼ í•¨
        # ê¸°ì¡´: top/u_cpu/* ë¡œ ì˜ë ¸ìŒ (Truncation)
        # ë³€ê²½: top/u_cpu/decode/pipe_* (Full Path ìœ ì§€)
        for i in range(10): f.write(f"LINT-01: Signal 'top/u_cpu/decode/pipe_{i}' float\n")
        for i in range(10): f.write(f"LINT-01: Signal 'top/u_cpu/execute/pipe_{i}' float\n")
        
        # [Case 2] ê¸€ìê°€ ë‹¤ë¥´ì§€ë§Œ ì˜ë¯¸ê°€ ê°™ì€ ê²½ìš° (AI ì—­í• )
        f.write("TIM-01: Path 'top/mem/ddr_phy_ctrl' violation\n")
        f.write("TIM-01: Path 'top/mem/ddr_controller' violation\n")

    print("ğŸš€ Pipeline Start\n")

    # 1. Read & Parse
    reader = SubutaiLogReader(log_filename)
    parser = SubutaiParser()
    parsed_logs = [parser.parse_line(line) for line in reader.stream_valid_lines()]
    
    # 2. Logic (Full Path with Masking)
    # ì ˆì‚­(Truncation) ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ìˆ«ìë§Œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
    logic_engine = LogicClusterer()
    logic_results = logic_engine.run(parsed_logs)
    print(f"âœ… Logic Result: {len(logic_results)} groups (Full Path Preserved)")
    
    # 3. AI (Semantic Merge)
    # ì‚´ì•„ìˆëŠ” Full Path ì •ë³´ë¥¼ ì´ìš©í•´ ì •í™•í•˜ê²Œ ë¬¶ìŠµë‹ˆë‹¤.
    ai_engine = AIClusterer()
    final_results = ai_engine.run(logic_results)
    
    # 4. Report
    print("\n" + "="*50)
    for group in final_results[:5]:
        print(f"[{group['representative_pattern']}] (Count: {group['total_count']})")
        if len(group['sub_patterns']) > 1:
            print(f"  â”” Merged: {[sub['pattern'] for sub in group['sub_patterns']]}")
    
    if os.path.exists(log_filename): os.remove(log_filename)