import os
import sys
import re
import json
import hashlib
import time
from collections import defaultdict

# ==============================================================================
# [Dependency Check]
# ==============================================================================
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.cluster import DBSCAN
    import numpy as np
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False

# ==============================================================================
# 1. Template Manager
# ==============================================================================
class RuleTemplateManager:
    def __init__(self, template_file):
        self.template_dict = {} 
        self.var_pattern = re.compile(r"'(.*?)'")
        
        if template_file:
            print(f"üìÇ Loading Rule Templates from: {template_file}")
            self._load_templates(template_file)

    def _get_pure_template(self, text):
        # 1. Î≥ÄÏàò ÏòÅÏó≠ Î≥¥Ìò∏
        temp = self.var_pattern.sub("'<VAR>'", text)
        # 2. ÎèÖÎ¶ΩÎêú Ïà´ÏûêÎßå ÎßàÏä§ÌÇπ
        temp = re.sub(r"\b\d+\b", "<NUM>", temp)
        return temp.strip()

    def _load_templates(self, file_path):
        if not os.path.exists(file_path):
            return
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith(('-', 'Rule', 'Severity')): continue
                parts = line.split(maxsplit=3)
                if len(parts) < 4: continue
                rule_id, message = parts[0], parts[3]
                pure_temp = self._get_pure_template(message)
                self.template_dict[pure_temp] = rule_id

    def get_rule_id(self, log_template):
        return self.template_dict.get(log_template, f"UNKNOWN_{hashlib.md5(log_template.encode()).hexdigest()[:6].upper()}")

# ==============================================================================
# 2. Parser
# ==============================================================================
class SubutaiParser:
    def __init__(self, template_manager):
        self.var_pattern = re.compile(r"'(.*?)'")
        self.tm = template_manager
        self.delimiters = [('/', 1), ('_', 2), ('-', 3)]  # (delimiter, priority)
    
    def extract_variable_stems(self, variable):
        """
        Extract semantic stems from variable respecting delimiter priority.
        Priority: '/' (highest) > '_' > '-' (lowest)
        
        Strategy: Split by priority delimiters but keep meaningful components.
        - '/' is hierarchy separator: splits into distinct components
        - '_' is compound separator within a component: may keep together or split
        - '-' is sub-component separator: splits into atoms
        
        Example: 'BLK_CPU/A/B/C/mem_top_ABC' -> ['BLK_CPU', 'A', 'B', 'C', 'mem_top', 'ABC']
        Example: 'mem_top_ABC' -> ['mem_top', 'ABC']
        
        Returns list of stem components in hierarchical order.
        """
        if not variable:
            return []
        
        # Step 1: Split by highest priority delimiter ('/')
        parts = variable.split('/')
        stems = []
        
        for part in parts:
            if not part:
                continue
            
            # Step 2: For each part, decide whether to split by '_' or '-'
            # Strategy: If the part is a known hierarchy marker (A, B, C, X, Y, etc.) or very short, keep it
            # Otherwise split by '_' (compound names like mem_top), then by '-'
            
            if len(part) <= 3 or part.isupper():
                # Single letters or uppercase markers like BLK, CPU, SENSOR - keep as one stem
                stems.append(part)
            else:
                # Compound names: split by '_' first, then '-'
                sub_parts = part.split('_')
                for sub_part in sub_parts:
                    if sub_part:
                        # Final split by '-' for components like 'ABC', '123-456'
                        final_parts = sub_part.split('-')
                        stems.extend([p for p in final_parts if p])
        
        return stems

    def parse_line(self, line):
        line = line.strip()
        if not line: return None
        
        if re.search(r'\b\d+\s+of\s+\d+\b', line):
            pass 
        else:
            return None

        line = " ".join(line.split()[4:])

        variables = self.var_pattern.findall(line)
        var_tuple = tuple(variables) if variables else ("NO_VAR",)
        
        # Extract variable stems for hierarchical grouping
        variable_stems = []
        if var_tuple and var_tuple != ("NO_VAR",):
            for var in var_tuple:
                stems = self.extract_variable_stems(var)
                variable_stems.extend(stems)
        
        stems_tuple = tuple(variable_stems) if variable_stems else ("NO_STEM",)
        
        template = self.tm._get_pure_template(line)
        rule_id = self.tm.get_rule_id(template)
        
        return {
            "rule_id": rule_id,
            "variables": var_tuple,
            "variable_stems": stems_tuple,
            "template": template,
            "raw_log": line  # <--- ÏõêÎ≥∏ Î°úÍ∑∏ Ï†ÄÏû•Îê®
        }

# ==============================================================================
# 3. Logic Clusterer
# ==============================================================================
class LogicClusterer:
    def get_logic_signature(self, var_tuple):
        if not var_tuple or var_tuple == ("NO_VAR",): return "NO_VAR"
        sigs = [re.sub(r"\d+", "*", str(v)) for v in var_tuple]
        return " / ".join(sigs)
    
    def get_stem_signature(self, stem_tuple):
        """
        Create signature from variable stems, replacing numbers with wildcards.
        Stems are already decomposed, so this focuses on semantic components.
        
        Example: ('mem_top', 'ABC', 'value', '123') -> 'mem_top ABC value *'
        """
        if not stem_tuple or stem_tuple == ("NO_STEM",): 
            return "NO_STEM"
        
        # Replace numeric stems with wildcard, keep semantic stems
        sigs = []
        for stem in stem_tuple:
            if stem.isdigit():
                sigs.append("*")
            else:
                # Keep non-numeric stems as-is (they're already atomic)
                sigs.append(stem)
        
        return " ".join(sigs)

    def run(self, parsed_logs):
        groups = defaultdict(list)
        for p in parsed_logs:
            stem_sig = self.get_stem_signature(p['variable_stems'])
            # Create composite key: prioritize stem signature for semantic grouping
            # but include full signature and template for complete context
            key = (p['rule_id'], stem_sig, p['template'])
            groups[key].append(p)

        results = []
        for (rule_id, stem_sig, temp), members in groups.items():
            # Calculate full pattern for reference
            full_sigs = [self.get_logic_signature(m['variables']) for m in members]
            full_sig_representative = full_sigs[0] if full_sigs else "NO_VAR"
            
            results.append({
                "type": "LogicGroup",
                "rule_id": rule_id,
                "stem_pattern": stem_sig,  # Primary pattern (hierarchical)
                "full_pattern": full_sig_representative,  # Reference pattern
                "template": temp,
                "count": len(members),
                "members": members  # <--- Ïó¨Í∏∞Ïóê raw_logÍ∞Ä Ìè¨Ìï®Îêú ÌååÏã± Í∞ùÏ≤¥Îì§Ïù¥ ÏûàÏùå
            })
        results.sort(key=lambda x: x['count'], reverse=True)
        return results

# ==============================================================================
# 4. AI Clusterer
# ==============================================================================
class AIClusterer:
    def __init__(self, model_path='all-MiniLM-L6-v2'):
        global AI_AVAILABLE
        if AI_AVAILABLE:
            try:
                self.model = SentenceTransformer(model_path)
            except:
                AI_AVAILABLE = False

    def run(self, logic_groups):
        if not AI_AVAILABLE or not logic_groups: return []

        print(f"ü§ñ AI analyzing {len(logic_groups)} logic groups...")
        # Use stem_pattern for embeddings to enable hierarchical similarity detection
        # This allows logs with different depths to be recognized as similar
        embedding_inputs = [f"{g['template']} {g['stem_pattern']}" for g in logic_groups]
        embeddings = self.model.encode(embedding_inputs, batch_size=128, show_progress_bar=False)
        
        # Increased eps from 0.2 to 0.3 for stem-based clustering
        # Higher eps allows more semantic flexibility when matching hierarchically different logs
        clustering = DBSCAN(eps=0.3, min_samples=1, metric='cosine').fit(embeddings)
        
        ai_grouped = defaultdict(lambda: {"total_count": 0, "logic_subgroups": []})
        for label, logic_group in zip(clustering.labels_, logic_groups):
            cluster_key = f"{logic_group['rule_id']}_SG_{label}"
            ai_grouped[cluster_key]["total_count"] += logic_group['count']
            ai_grouped[cluster_key]["logic_subgroups"].append(logic_group)

        final_output = []
        for key, data in ai_grouped.items():
            main = max(data["logic_subgroups"], key=lambda x: x['count'])
            
            # [ÌïµÏã¨] ÏõêÎ≥∏ Î°úÍ∑∏ Î≥µÍµ¨ Î°úÏßÅ
            # AI Í∑∏Î£π -> Logic ÏÑúÎ∏åÍ∑∏Î£π -> Î©§Î≤Ñ -> raw_log ÏàúÏúºÎ°ú Ï∂îÏ∂úÌïòÏó¨ Ìï©Ïπ®
            all_raw_logs = []
            for sub in data["logic_subgroups"]:
                for member in sub["members"]:
                    all_raw_logs.append(member["raw_log"])

            final_output.append({
                "type": "AISuperGroup",
                "super_group_id": key,
                "rule_id": main['rule_id'],
                "representative_template": main['template'],
                "representative_stem_pattern": main['stem_pattern'],  # Hierarchical pattern (primary)
                "representative_full_pattern": main['full_pattern'],  # Full variable pattern (reference)
                "total_count": data["total_count"],
                "merged_variants_count": len(data["logic_subgroups"]),
                "original_logs": all_raw_logs  # <--- Î≥µÍµ¨Îêú ÏõêÎ≥∏ Î°úÍ∑∏ Î¶¨Ïä§Ìä∏
            })
        
        final_output.sort(key=lambda x: x['total_count'], reverse=True)
        return final_output

# ==============================================================================
# 5. Main Execution
# ==============================================================================
if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python subutai_reviewer.py <LOG_FILE> <TEMPLATE_FILE>")
        sys.exit(1)

    log_file = sys.argv[1]
    rule_file = sys.argv[2]

    # 1. Parsing
    tm = RuleTemplateManager(rule_file)
    parser = SubutaiParser(tm)
    parsed_logs = []
    
    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            stripped = line.strip()
            if not stripped or stripped.startswith(('-', '=', 'Rule', 'Severity')): continue
            res = parser.parse_line(stripped)
            if res: parsed_logs.append(res)

    # 2. Logic Clustering
    logic_results = LogicClusterer().run(parsed_logs)

    # 3. AI Clustering & Result Aggregation
    results = [] # <--- Ïó¨Í∏∞Ïóê Î™®Îì† Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï©ÎãàÎã§.

    if AI_AVAILABLE:
        # AI Í≤∞Í≥ºÏóêÎäî Ïù¥ÎØ∏ original_logs Î≥µÍµ¨ Î°úÏßÅÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏùå
        results = AIClusterer().run(logic_results)
    else:
        # AIÍ∞Ä ÏóÜÏúºÎ©¥ Logic Í≤∞Í≥ºÎ•º Ìè¨Îß∑ÌåÖÌïòÏó¨ Ï†ÄÏû•
        for g in logic_results:
            # Logic Í∑∏Î£πÏùò ÏõêÎ≥∏ Î°úÍ∑∏ Î≥µÍµ¨
            raw_logs = [m['raw_log'] for m in g['members']]
            results.append({
                "type": "LogicGroup",
                "rule_id": g['rule_id'],
                "representative_stem_pattern": g['stem_pattern'],  # Hierarchical pattern (primary)
                "representative_full_pattern": g['full_pattern'],  # Full variable pattern (reference)
                "total_count": g['count'],
                "original_logs": raw_logs
            })

    # 4. Í≤∞Í≥º Ï∂úÎ†• Î∞è ÌååÏùº Ï†ÄÏû•
    print("\n" + "="*80)
    print(f"‚úÖ Final Results: {len(results)} Groups Created.")
    print("="*80)
    
    # ÌôîÎ©¥ Ï∂úÎ†• (ÏÉòÌîå)
    for i, res in enumerate(results[:5]):
        pattern_display = res.get('representative_stem_pattern', res.get('representative_pattern', 'N/A'))
        print(f"{i+1:02d}. [{res['rule_id']}] {pattern_display}")
        print(f"    Count: {res['total_count']:,}")
        print(f"    Original Logs Sample (Top 2):")
        for log in res['original_logs'][:2]:
            print(f"      - {log}")
        print("-" * 60)

    # Í≤∞Í≥º ÌååÏùº Ï†ÄÏû• (JSON)
    output_filename = "subutai_results.json"
    with open(output_filename, "w", encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Î™®Îì† Í≤∞Í≥º(ÏõêÎ≥∏ Î°úÍ∑∏ Ìè¨Ìï®)Í∞Ä '{output_filename}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")